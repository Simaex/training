{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling Walkthrough - 20 newsgroups\n",
    "\n",
    "This walkthrough will take you through a simple topic modeling task. By the end of the tutorial, you should be able to create a simple machine learning workflow to perform topic modeling for a set of email groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a project\n",
    "\n",
    "To start, we'll quickly create a Squirro project that we can work in. To do this you'll need a running Squirro cluster and a valid API token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER = \"\"\n",
    "TOKEN = \"\"\n",
    "\n",
    "# get a client\n",
    "from squirro_client import SquirroClient\n",
    "client = SquirroClient(client_id=None, client_secret=None, cluster=CLUSTER)\n",
    "client.authenticate(refresh_token=TOKEN)\n",
    "\n",
    "# create a project\n",
    "PROJECT_ID = client.new_project(\"Topic Modeling Walkthrough\").get(\"id\")\n",
    "print PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "The next step is to load data in our Squirro instance. We can now run a pre-made Squirro data loader script to insert our data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "print subprocess.check_output([\"./load.sh\", CLUSTER, TOKEN, PROJECT_ID], stderr=subprocess.STDOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the dataset\n",
    "\n",
    "Just as in the classification walkthrough, we first want to get an idea of what is in our dataset, so we print a single item to look at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print a single item\n",
    "item = client.query(project_id=PROJECT_ID, query='label:*',\n",
    "                    fields=['body','title','keywords'], count=1)['items'][0]\n",
    "print u'{label} - {title} - {body}'.format(\n",
    "    title=item['title'], body=item['body'], label=item['keywords']['label'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the items in our dataset are group emails, so they include quite a bit of natural language.\n",
    "\n",
    "Next we'll look at the dataset as a whole to get an idea of the balance between the newsgroups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = client.query(project_id=PROJECT_ID, query='*', aggregations={'label': {}})\n",
    "for value in res['aggregations']['label']['label']['values']:\n",
    "    print u'{label} - {count}'.format(label=value['key'], count=value['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our dataset has about 1K samples per label, and only 3 labels. The labels represent the topic of each newsgroup, and here the topics are different enough that topic modeling should be effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model workflow\n",
    "\n",
    "Now that we have an idea of the data we're dealing with, we can move on to building our topic model. To reiterate the goal, we want to build a model that determines the topics in our dataset automatically, without telling it ahead of time what the categories should be (this is an unsupervised learning task).\n",
    "\n",
    "The heart of Squirro's Machine Learning Service is our custom natural language processing library libNLP. It is what actually does all the processing. Thus our model workflow is simply a libNLP workflow, which we'll walk through now. (For extended documentation for libNLP, see https://squirro.github.io/nlp/).\n",
    "\n",
    "The libNLP workflow is simply a JSON file with specifications for individual components required for machine learning, so we start with an empty JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the dataset\n",
    "\n",
    "The first thing we need to do is tell libNLP on which dataset to operate. We do this by providing Squirro queries to `train`, `test`, and `infer` data sets. `train` is the data we want to train the model on. `test` is the data we'd like to test the model on, and `infer` is the data we'd like to predict on (which is typically unlabeled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow[\"dataset\"] = {\n",
    "    \"train\": {\"query_string\": \"dataset:train\"},\n",
    "    \"test\": {\"query_string\": \"dataset:test\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have already split our dataset into a training and test set using a `dataset` facet during loading. Notice also that `query_string` can be any Squirro query, making it easy to carve out your samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the analyzer\n",
    "\n",
    "Next we want to tell libNLP the type of machine learning task we have. That way we can later analyze how well we are doing at this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow[\"analyzer\"] = {\n",
    "    \"type\": \"topic_modeling\",\n",
    "    \"label_field\": \"keywords.label\",\n",
    "    \"tag_field\": \"keywords.topics\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we said we have a `topic_modeling` task, where the ground-truth label is `label` and the field with our create topics is `topics`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the pipeline\n",
    "\n",
    "Finally we need to tell libNLP the steps we'll use to go from unstructured text to a prediction for each item. We do so by defining a pipeline compose of sequential steps where each step does some operation on an internal stream of items.\n",
    "\n",
    "Here we only present the steps that we need for this task. For a list of all steps and associated documentation, see https://squirro.github.io/nlp/.\n",
    "\n",
    "First we instantiate an empty pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow[\"pipeline\"] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loader step\n",
    "\n",
    "The first step is to load the data from Squirro into libNLP and convert them to libNLP's internal format. This step will be passed the various `dataset` settings we gave above since it is the beginning of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow['pipeline'].append({\n",
    "    \"step\": \"loader\",\n",
    "    \"type\": \"squirro_query\",\n",
    "    \"fields\": [\"body\", \"title\", \"keywords.label\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we specified the `fields` we wanted to import to make loading more efficient.\n",
    "\n",
    "Also note, that when the loader step gets content, it will always turn it into a flat dictionary before passing it to the next step in the pipeline. This is why we prepend `keywords.` to the fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter steps\n",
    "\n",
    "Next we want to filter down our dataset, and adjust its format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty entries\n",
    "workflow['pipeline'].append({\n",
    "    \"step\": \"filter\",\n",
    "    \"type\": \"empty\",\n",
    "    \"fields\": [\"title\", \"body\"]\n",
    "})\n",
    "\n",
    "# merge title and body into 1 field\n",
    "workflow['pipeline'].append({\n",
    "    \"step\": \"filter\",\n",
    "    \"type\": \"merge\",\n",
    "    \"input_fields\": [\"title\", \"body\"],\n",
    "    \"output_field\": \"text\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these 2 filters we have removed all items that are missing `title` or `body`, and merged `body` and `title` into a single field called `text`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization step\n",
    "\n",
    "We next need to normalize the incoming data so that all the training samples are in the same format. This makes training the model simpler since it shrinks the space of data it has to be able to predict on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow['pipeline'].append({\n",
    "    \"step\": \"normalizer\",\n",
    "    \"types\": [\"html\", \"character\", \"punctuation\", \"lowercase\", \"stopwords\"],\n",
    "    \"fields\": [\"text\"],\n",
    "    \"stopwords\": [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"b\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"c\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"gt\", \"had\", \"has\", \"have\", \"having\", \"he\", \"hed\", \"hell\", \"hes\", \"her\", \"here\", \"heres\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"hows\", \"i\", \"id\", \"ill\", \"im\", \"ive\", \"if\", \"in\", \"into\", \"is\", \"it\", \"its\", \"its\", \"itself\", \"lets\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"not\", \"o\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"q\", \"re\", \"s\", \"same\", \"she\", \"shed\", \"shell\", \"shes\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"thats\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"theres\", \"these\", \"they\", \"theyd\", \"theyll\", \"theyre\", \"theyve\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"v\", \"very\", \"was\", \"we\", \"wed\", \"well\", \"were\", \"weve\", \"were\", \"what\", \"whats\", \"when\", \"whens\", \"where\", \"wheres\", \"which\", \"while\", \"who\", \"whos\", \"whom\", \"why\", \"whys\", \"with\", \"would\", \"x\", \"you\", \"youd\", \"youll\", \"youre\", \"youve\", \"your\", \"yours\", \"yourself\", \"yourselves\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here for the field `body`, we are first stripping out `html`, numeric `character`s, and `punctuation`, and then making everything `lowercase`. Finally, we remove `stopwords` for which we provided a list of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization step\n",
    "\n",
    "Now we need to split our input from a stream of words into a list of tokens. For this particular case, we can use the `spaces` tokenizer to get our a sequential list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow['pipeline'].append({\n",
    "    \"step\": \"tokenizer\",\n",
    "    \"type\": \"spaces\",\n",
    "    \"fields\": [\"text\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding step\n",
    "\n",
    "Right before classification, we have to convert our list of tokenized words into numbers. This is done via an `embedder` step. Squirro comes shipped with some pre-trained embeddings, but for this case, we'll make our own TF-IDF embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow['pipeline'].append({\n",
    "    \"step\": \"embedder\",\n",
    "    \"type\": \"tfidf\",\n",
    "    \"input_field\": \"text\",\n",
    "    \"output_field\": \"embedded_text\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Projection step\n",
    "\n",
    "The previous step produces very large vectors (essentially the size is the number of unique words in your corpus). To reduce this space, it can be helpful to do a projection down to a smaller vector space. That is exactly what a projection step accomplishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow['pipeline'].append({\n",
    "    \"step\": \"projector\",\n",
    "    \"type\": \"sklearn\",\n",
    "    \"model_type\": \"svd\",\n",
    "    \"n_components\": 100,\n",
    "    \"input_field\": \"embedded_text\",\n",
    "    \"output_field\": \"embedded_text\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've chosen to reduce the vector size to 100 by using an `svd` projector from scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering step\n",
    "\n",
    "We are now ready to cluster the incoming items (which are now represented as numerical vectors). For this task we'll use a Gaussian Mixture Model clusterer from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow['pipeline'].append({\n",
    "    \"step\": \"clusterer\",\n",
    "    \"type\": \"gmm\",\n",
    "    \"n_clusters\": 3,\n",
    "    \"input_field\": \"embedded_text\",\n",
    "    \"output_field\": \"keywords.topics\",\n",
    "    \"explanation_field\": \"keywords.sig_terms\",\n",
    "    \"term_field\": \"text\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This clusterer takes our input field `embedded_text` and attempts to predict the cluster all the items into up to 3 clusters. We will write the name of these clusters to `keywords.topics`. Also on each item, we'll provide a list of significant terms in `keywords.sig_terms`. These make up the core of each topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saver step\n",
    "\n",
    "Finally we want to save our predictions back to Squirro. We do this through a saver step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow[\"pipeline\"].append({\n",
    "    \"step\": \"saver\",\n",
    "    \"type\": \"squirro_item\",\n",
    "    \"fields\": [\"keywords.topics\", \"keywords.sig_terms\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that only the fields we specify in `fields` will be sent back to Squirro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All together\n",
    "\n",
    "Putting it all together, our libNLP workflow looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "print json.dumps(workflow, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Now we're ready to train our proposed workflow. To do that we can simply push it to the Squirro Machine Learning Service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_workflow_id = client.new_machinelearning_workflow(\n",
    "    PROJECT_ID, name='20_newsgroups', config=workflow).get('id')\n",
    "print ml_workflow_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a training job for the workflow. This will tell the Machine Learning Service to schedule a job that runs the workflow with the `train` dataset we specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_id = client.new_machinelearning_job(\n",
    "    PROJECT_ID, ml_workflow_id=ml_workflow_id, type='training').get('id')\n",
    "print training_job_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just wait for it to finish. Depending on the size the dataset, size of the model, and the number of free parameters, this can take anywhere from a few seconds to days. Because of this, it's always a good idea to START SMALL with a test dataset and model until you're confident things are working well.\n",
    "\n",
    "Since training will take up to 5 minutes to finish, we write the simple function below that pings the job status every 5 seconds. Once this cell is done evaluating, we'll be ready to move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def wait_for_ml_job(project_id, ml_workflow_id, ml_job_id, max_wait_time=600):\n",
    "    \"\"\"Wait for ML job to finish\"\"\"\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        job = client.get_machinelearning_job(\n",
    "            project_id, ml_workflow_id, ml_job_id, include_run_log=True).get('machinelearning_job')\n",
    "        if job.get('last_error_at') is not None or job.get('last_success_at') is not None:\n",
    "            print job.get('logs')\n",
    "            break\n",
    "        else:\n",
    "            print '.',\n",
    "            time.sleep(5)\n",
    "        if (time.time() - start_time) > max_wait_time:\n",
    "            print 'max_wait_time has been exceeded!'\n",
    "            print job.get('logs')\n",
    "            break\n",
    "wait_for_ml_job(PROJECT_ID, ml_workflow_id, training_job_id, max_wait_time=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the model quality\n",
    "\n",
    "Now that our model is trained, we can check out how it performed on our test data set (again in this instance it was the same as the training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.get_machinelearning_job(\n",
    "    PROJECT_ID, ml_workflow_id, training_job_id).get('machinelearning_job').get('last_result')\n",
    "print json.dumps(result, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results tell us several things. First `tag_counts` shows us how each ground truth label corresponds to different generate topics. We can see for each label, there is one topic with the majority of the spectral weight. The title for the topic has been generated as well, taking significant terms from a TF-IDF calculation.\n",
    "\n",
    "Also presented are 3 scores. The `homogeneity_score` measures for each topic, the fraction of the majority label and averages them together. The `completeness_score` measures for each label, the fraction that belongs to the majority topic and averages them together. Finally the `adjusted_rand_score` is a measure of the dissimilarity of the generated topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the model on new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our model has reasonble (though not perfect) quality, we can now move on to validating it on samples that don't yet have a `label`. We can do this in a few different ways, which we cover below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct inference\n",
    "\n",
    "First, it's good to do a sanity check. The simplest way to check our model on new data is to run a direct inference on items we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_items = [{\"id\": 0, \"title\": \"God is dead\", \"body\": \"There is no god, I'm an atheist.\"},\n",
    "              {\"id\": 1, \"title\": \"Jesus saves\", \"body\": \"Jesus washes away all our sins.\"},\n",
    "              {\"id\": 2, \"title\": \"Video games for sale\", \"body\": \"Ocarina of time is the greatest game ever made, and I'm selling the gold catridge version!\"}]\n",
    "\n",
    "test_items_pred = client.run_machinelearning_workflow(\n",
    "    PROJECT_ID, ml_workflow_id, data={'items': test_items})['items']\n",
    "for item, item_pred in zip(test_items, test_items_pred):\n",
    "    print u'{topics} - {title}'.format(title=item['title'], topics=item_pred['keywords']['topics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems reasonable..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a pipelet to for ingestion\n",
    "\n",
    "Now that we have some confidence in our trained model, we can set up a pipelet step that will run items through it during ingestion. For this we have made an example pipelet here: https://github.com/squirro/delivery/tree/master/templates/pipelets/machinelearning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add an inference job for future data\n",
    "\n",
    "If we want to avoid blocking the ingestion process, we can instead make an ayschronous inference job that will tag new items with our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_job_id = client.new_machinelearning_job(\n",
    "    PROJECT_ID, ml_workflow_id=ml_workflow_id, type='inference', scheduling_options={}).get('id')\n",
    "print inference_job_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset\n",
    "\n",
    "WARNING: This deletes the project!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_project(PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
