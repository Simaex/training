{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Walkthrough - Forward Interest\n",
    "\n",
    "This walkthrough will take you through a simple classification task. By the end of the tutorial, you should be able to create a simple machine learning workflow to classify sentences for `forward interest`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a project\n",
    "\n",
    "To start, we'll quickly create a Squirro project that we can work in. To do this you'll need a running Squirro cluster and a valid API token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER = \"\"\n",
    "TOKEN = \"\"\n",
    "\n",
    "# get a client\n",
    "from squirro_client import SquirroClient\n",
    "client = SquirroClient(client_id=None, client_secret=None, cluster=CLUSTER)\n",
    "client.authenticate(refresh_token=TOKEN)\n",
    "\n",
    "# create a project\n",
    "PROJECT_ID = client.new_project(\"Classification Walkthrough\").get(\"id\")\n",
    "print PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "The next step is to load data in our Squirro instance. We can now run a pre-made Squirro data loader script to insert our data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{\"./load.sh %s %s %s\" % (CLUSTER, TOKEN, PROJECT_ID)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the dataset\n",
    "\n",
    "The first step in any machine learning project should be to look carefully at your dataset. Try to answer questions like:\n",
    "- How many labeled samples do I have?\n",
    "- Are the labels evenly distributed between categories?\n",
    "- How accurate would I be if I labeled the samples randomly?\n",
    "- How accurate would I be if I labeled all the samples as only one category?\n",
    "Answering these questions will give you an idea of what method to use, what parameters to use for that method, and what the baseline perfomance might be.\n",
    "\n",
    "For the dataset we just loaded, we'll first look at a few samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a positive item\n",
    "for item in client.query(project_id=PROJECT_ID,\n",
    "                         query='dataset:train label:pos',\n",
    "                         fields=['body','keywords'], count=1)['items']:\n",
    "    print u'{label} - {body}'.format(body=item['body'], label=item['keywords']['label'][0])\n",
    "    \n",
    "# print a negative item\n",
    "for item in client.query(project_id=PROJECT_ID,\n",
    "                         query='dataset:train label:neg',\n",
    "                         fields=['body','keywords'], count=1)['items']:\n",
    "    print u'{label} - {body}'.format(body=item['body'], label=item['keywords']['label'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we've printed a single positive and negative example to get an idea of what we're looking for. No big surprises so far.\n",
    "\n",
    "Next we'll look at the dataset as a whole to get an idea of the balance between the two labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = client.query(project_id=PROJECT_ID, query='*', aggregations={'label': {}})\n",
    "for value in res['aggregations']['label']['label']['values']:\n",
    "    print u'{label} - {count}'.format(label=value['key'], count=value['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, while our dataset has a decent number of samples (almost 18K), it has many more negative examples than positive. This imbalance should be noted, as it tells us a couple things. First, if we were to guess every item was negative, we'd already be ~85% correct! This is considerably higher than the random guessing baseline of 50%. Second, this imbalance might skew our model towards the negative category. To account for this, we should consider weighting the model categories when we construct our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model workflow\n",
    "\n",
    "Now that we have an idea of the data we're dealing with, we can move on to building our classification model. To reiterate the goal, we want to build a model that can guess a whether or not a sentence is forward-looking based on the text therein.\n",
    "\n",
    "The heart of Squirro's Machine Learning Service is our custom natural language processing library libNLP. It is what actually does all the processing. Thus our model workflow is simply a libNLP workflow, which we'll walk through now. (For extended documentation for libNLP, see https://squirro.github.io/nlp/).\n",
    "\n",
    "The libNLP workflow is simply a JSON file with specifications for individual components required for machine learning, so we start with an empty JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the dataset\n",
    "\n",
    "The first thing we need to do is tell libNLP on which dataset to operate. We do this by providing Squirro queries to `train`, `test`, and `infer` data sets. `train` is the data we want to train the model on. `test` is the data we'd like to test the model on, and `infer` is the data we'd like to predict on (which is typically unlabeled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow[\"dataset\"] = {\n",
    "    \"train\": {\"query_string\": \"dataset:train\"},\n",
    "    \"test\": {\"query_string\": \"dataset:test\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have already split our dataset into a training and test set using a `dataset` facet during loading. Notice also that `query_string` can be any Squirro query, making it easy to carve out your samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the analyzer\n",
    "\n",
    "Next we want to tell libNLP the type of machine learning task we have. That way we can later analyze how well we are doing at this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow[\"analyzer\"] = {\n",
    "    \"type\": \"classification\",\n",
    "    \"label_field\": \"keywords.label\",\n",
    "    \"tag_field\": \"keywords.label_pred\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we said we have a `classification` task, where the ground-truth label is `label` and the field with our predicted gender is `label_pred`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the pipeline\n",
    "\n",
    "Finally we need to tell libNLP the steps we'll use to go from unstructured text to a prediction for each item. We do so by defining a pipeline compose of sequential steps where each step does some operation on an internal stream of items.\n",
    "\n",
    "Here we only present the steps that we need for this task. For a list of all steps and associated documentation, see https://squirro.github.io/nlp/.\n",
    "\n",
    "First we instantiate an empty pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow[\"pipeline\"] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loader step\n",
    "\n",
    "The first step is to load the data from Squirro into libNLP and convert them to libNLP's internal format. This step will be passed the various `dataset` settings we gave above since it is the beginning of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow['pipeline'].append({\n",
    "    \"step\": \"loader\",\n",
    "    \"type\": \"squirro_query\",\n",
    "    \"fields\": [\"body\", \"keywords.label\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we specified the `fields` we wanted to import to make loading more efficient.\n",
    "\n",
    "Also note, that when the loader step gets content, it will always turn it into a flat dictionary before passing it to the next step in the pipeline. This is why we prepend `keywords.` to the fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization step\n",
    "\n",
    "We next need to normalize the incoming data so that all the training samples are in the same format. This makes training the model simpler since it shrinks the space of data it has to be able to predict on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow['pipeline'].append({\n",
    "    \"step\": \"normalizer\",\n",
    "    \"types\": [\"html\", \"character\", \"punctuation\", \"lowercase\"],\n",
    "    \"fields\": [\"body\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here for the field `body`, we are first stripping out `html`, numeric `character`s, and `punctuation`, and then making everything `lowercase`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization step\n",
    "\n",
    "Now we need to split our input from a stream of words into a list of tokens. For this particular case, we can use the `spaces` tokenizer to get our a sequential list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow['pipeline'].append({\n",
    "    \"step\": \"tokenizer\",\n",
    "    \"type\": \"spaces\",\n",
    "    \"fields\": [\"body\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding step\n",
    "\n",
    "Right before classification, we have to convert our list of tokenized words into numbers. This is done via an `embedder` step. Squirro comes shipped with some pre-trained embeddings, but for this case, we'll make our own TF-IDF embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow['pipeline'].append({\n",
    "    \"step\": \"embedder\",\n",
    "    \"type\": \"tfidf\",\n",
    "    \"input_field\": \"body\",\n",
    "    \"output_field\": \"embedded_body\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification step\n",
    "\n",
    "We are now ready to classify the incoming items. For this task we'll use a simple SVM classifier from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow['pipeline'].append({\n",
    "    \"step\": \"classifier\",\n",
    "    \"type\": \"sklearn\",\n",
    "    \"model_type\": \"SVC\",\n",
    "    \"model_kwargs\": {\"probability\": True},\n",
    "    \"use_sparse\": True,\n",
    "    \"input_fields\": [\"embedded_body\"],\n",
    "    \"label_field\": \"keywords.label\",\n",
    "    \"output_field\": \"keywords.label_pred\",\n",
    "    \"explanation_field\": \"keywords.label_pred_explanation\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifier takes our input field `embedded_body` and attempts to predict the label field `keywords.label`. It writes its prediction in the output field `label_pred`.\n",
    "\n",
    "Some models also provide an explanation of their prediction (though the SVM does not). Here it's written to `keywords.label_pred_explanation`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saver step\n",
    "\n",
    "Finally we want to save our predictions back to Squirro. We do this through a saver step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow[\"pipeline\"].append({\n",
    "    \"step\": \"saver\",\n",
    "    \"type\": \"squirro_item\",\n",
    "    \"fields\": [\"keywords.label_pred\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that only the fields we specify in `fields` will be sent back to Squirro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All together\n",
    "\n",
    "Putting it all together, our libNLP workflow looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "print json.dumps(workflow, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Now we're ready to train our proposed workflow. To do that we can simply push it to the Squirro Machine Learning Service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_workflow_id = client.new_machinelearning_workflow(\n",
    "    PROJECT_ID, name='gender_divide', config=workflow).get('id')\n",
    "print ml_workflow_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a training job for the workflow. This will tell the Machine Learning Service to schedule a job that runs the workflow with the `train` dataset we specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_id = client.new_machinelearning_job(\n",
    "    PROJECT_ID, ml_workflow_id=ml_workflow_id, type='training').get('id')\n",
    "print training_job_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just wait for it to finish. Depending on the size the dataset, size of the model, and the number of free parameters, this can take anywhere from a few seconds to days. Because of this, it's always a good idea to START SMALL with a test dataset and model until you're confident things are working well.\n",
    "\n",
    "Since training will take up to 5 minutes to finish, we write the simple function below that pings the job status every 5 seconds. Once this cell is done evaluating, we'll be ready to move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def wait_for_ml_job(project_id, ml_workflow_id, ml_job_id, max_wait_time=600):\n",
    "    \"\"\"Wait for ML job to finish\"\"\"\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        job = client.get_machinelearning_job(\n",
    "            project_id, ml_workflow_id, ml_job_id, include_run_log=True).get('machinelearning_job')\n",
    "        if job.get('last_error_at') is not None or job.get('last_success_at') is not None:\n",
    "            print job.get('logs')\n",
    "            break\n",
    "        else:\n",
    "            print '.',\n",
    "            time.sleep(5)\n",
    "        if (time.time() - start_time) > max_wait_time:\n",
    "            print 'max_wait_time has been exceeded!'\n",
    "            print job.get('logs')\n",
    "            break\n",
    "wait_for_ml_job(PROJECT_ID, ml_workflow_id, training_job_id, max_wait_time=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the model quality\n",
    "\n",
    "Now that our model is trained, we can check out how it performed on our test data set (again in this instance it was the same as the training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.get_machinelearning_job(\n",
    "    PROJECT_ID, ml_workflow_id, training_job_id).get('machinelearning_job').get('last_result')\n",
    "print json.dumps(result, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results tell us several things. First, we see the `precision` and `recall` of each predicted label. `precision` is the number of true positives divided by the total number of predictions for each category. `recall` is the number of true positives divided by the total number of samples for each category.\n",
    "\n",
    "We also see the `confusion matrix` which shows us where we are most likely to mis-predict. In a perfect classifier, only the diagonal of the matrix would be populated. Here we see, though, that there is population is off-diagonal elements as well, meaning we are mis-predicting in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the model on new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our model has reasonble (though not perfect) quality, we can now move on to validating it on samples that don't yet have a `label`. We can do this in a few different ways, which we cover below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct inference\n",
    "\n",
    "First, it's good to do a sanity check. The simplest way to check our model on new data is to run a direct inference on items we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_items = [{\"id\": 0, \"body\": \"The board gives us its full support.\"},\n",
    "              {\"id\": 1, \"body\": \"We will make sure we have enough runway for the next 12 months.\"},\n",
    "              {\"id\": 2, \"body\": \"We are actively looking for investment.\"},\n",
    "              {\"id\": 3, \"body\": \"Luke, I am your father.\"}]\n",
    "\n",
    "test_items_pred = client.run_machinelearning_workflow(\n",
    "    PROJECT_ID, ml_workflow_id, data={'items': test_items}).get('items')\n",
    "for item, item_pred in zip(test_items, test_items_pred):\n",
    "    print u'{label} - {body}'.format(body=item['body'], label=item_pred['keywords']['label_pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems reasonable..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a pipelet to for ingestion\n",
    "\n",
    "Now that we have some confidence in our trained model, we can set up a pipelet step that will run items through it during ingestion. For this we have made an example pipelet here: https://github.com/squirro/delivery/tree/master/templates/pipelets/machinelearning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add an inference job for future data\n",
    "\n",
    "If we want to avoid blocking the ingestion process, we can instead make an ayschronous inference job that will tag new items with our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_job_id = client.new_machinelearning_job(\n",
    "    PROJECT_ID, ml_workflow_id=ml_workflow_id, type='inference', scheduling_options={}).get('id')\n",
    "print inference_job_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset\n",
    "\n",
    "WARNING: This deletes the project!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_project(PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
